{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import bs4\n",
    "import sys, os\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Files from the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The requests module was written because Python’s urllib2 module is\n",
    "too complicated to use. In fact, take a permanent marker and black out this\n",
    "entire paragraph. Forget I ever mentioned urllib2. If you need to download\n",
    "things from the web, just use the requests module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rj_address = 'https://automatetheboringstuff.com/files/rj.txt'\n",
    "rj_file = 'rj.txt'\n",
    "# Download the file\n",
    "rj = requests.get(rj_address)\n",
    "# Check if the download was successful\n",
    "rj.raise_for_status()\n",
    "# Save the file to disk in binary mode\n",
    "with open(rj_file, 'wb') as f:\n",
    "    f.write(rj.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n"
     ]
    }
   ],
   "source": [
    "# Print a few first lines of the file\n",
    "with open(rj_file, 'r') as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline(), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML with the bs4 Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- This is the example.html example file. -->\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   The Website Title\n",
      "  </ti\n"
     ]
    }
   ],
   "source": [
    "filename = 'example.html'\n",
    "# Load the file and parse it with BeautifulSoup\n",
    "with open(filename , 'r') as f:\n",
    "    soup = bs4.BeautifulSoup(f, 'html.parser')\n",
    "    # Print the first 100 characters of the parsed file \n",
    "    print(soup.prettify()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding an Element with the `select()` Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of author: <class 'bs4.element.ResultSet'>\n",
      "Length of author: 1\n",
      "Type of the first element: <class 'bs4.element.Tag'>\n",
      "\n",
      "Author as a string: <span id=\"author\">Al Sweigart</span>\n",
      "Author text: Al Sweigart\n",
      "Author attributes: {'id': 'author'}\n"
     ]
    }
   ],
   "source": [
    "# Load the file and parse it with BeautifulSoup\n",
    "with open(filename , 'r') as f:\n",
    "    soup = bs4.BeautifulSoup(f, 'html.parser')\n",
    "    # Find the element with an id attribute of author\n",
    "    authors = soup.select('#author')\n",
    "\n",
    "    # Check author type and length, check the type of the first element\n",
    "    print(f\"Type of author: {type(authors)}\")\n",
    "    print(f\"Length of author: {len(authors)}\")\n",
    "    print(f\"Type of the first element: {type(authors[0])}\\n\")\n",
    "\n",
    "    # Print the author element\n",
    "    author = authors[0]\n",
    "    # Print the author element as a string\n",
    "    print(f\"Author as a string: {str(author)}\")\n",
    "    # Print the text of the author element\n",
    "    print(f\"Author text: {author.getText()}\")\n",
    "    # Print the author element attributes\n",
    "    print(f\"Author attributes: {author.attrs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download my Python book from my website.\n",
      "Learn Python the easy way!\n",
      "By Al Sweigart\n"
     ]
    }
   ],
   "source": [
    "# Load the file and parse it with BeautifulSoup\n",
    "with open(filename, 'r') as f:\n",
    "    soup = bs4.BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "    # Pull all the <p> elements from the parsed file\n",
    "    paragraphs = soup.select('p')\n",
    "\n",
    "    # Print the first 3 paragraphs using getText()\n",
    "    for paragraph in paragraphs:\n",
    "        print(paragraph.getText())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data from an Element’s Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span element: <span id=\"author\">Al Sweigart</span>\n",
      "Span attributes: {'id': 'author'}\n",
      "Span 'id' attribute: 'author'\n"
     ]
    }
   ],
   "source": [
    "# Load the file and parse it with BeautifulSoup\n",
    "with open(filename, 'r') as f:\n",
    "    soup = bs4.BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "    # Pull all the <span> elements from the parsed file\n",
    "    spans = soup.select('span')\n",
    "\n",
    "    # Print the first span element and its  attributes\n",
    "    span = spans[0]\n",
    "    print(f\"Span element: {span}\")\n",
    "    print(f\"Span attributes: {span.attrs}\")\n",
    "\n",
    "    # Use get method to access the value of the id attribute\n",
    "    print(f\"Span 'id' attribute: '{span.get('id')}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Opening All Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get search results from Google for a specific python package\n",
    "url = \"https://pypi.org/search/?q=numpy\"\n",
    "result = requests.get(url)\n",
    "result.raise_for_status()\n",
    "\n",
    "# Print the first 100 characters of the search results\n",
    "# print(len(result.text))\n",
    "\n",
    "# Parse the search results with BeautifulSoup to get top 5 links\n",
    "soup = bs4.BeautifulSoup(result.text, 'html.parser')\n",
    "links = soup.select('.package-snippet')\n",
    "for link in links[:2]:\n",
    "    # Open the top 2 links in the browser\n",
    "    link_url = 'https://pypi.org' + link.get('href')\n",
    "    # webbrowser.open(link_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Downloading All XKCD Comics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading page https://xkcd.com...\n",
      "Image: <img alt=\"Ferris Wheels\" src=\"//imgs.xkcd.com/comics/ferris_wheels.png\" srcset=\"//imgs.xkcd.com/comics/ferris_wheels_2x.png 2x\" style=\"image-orientation:none\" title=\"They left the belt drive in place but switched which wheel was powered, so people could choose between a regular ride, a long ride, and a REALLY long ride.\"/>\n",
      "Image attr: {'src': '//imgs.xkcd.com/comics/ferris_wheels.png', 'title': 'They left the belt drive in place but switched which wheel was powered, so people could choose between a regular ride, a long ride, and a REALLY long ride.', 'alt': 'Ferris Wheels', 'srcset': '//imgs.xkcd.com/comics/ferris_wheels_2x.png 2x', 'style': 'image-orientation:none'}\n",
      "Downloading image https://imgs.xkcd.com/comics/ferris_wheels.png...\n",
      "\n",
      "Downloading page https://xkcd.com/2972/...\n",
      "Image: <img alt=\"Helium Synthesis\" src=\"//imgs.xkcd.com/comics/helium_synthesis.png\" srcset=\"//imgs.xkcd.com/comics/helium_synthesis_2x.png 2x\" style=\"image-orientation:none\" title=\"Our lawyers were worried because it turns out the company inherits its debt from the parent universe, but luckily cosmic inflation reduced it to nearly zero.\"/>\n",
      "Image attr: {'src': '//imgs.xkcd.com/comics/helium_synthesis.png', 'title': 'Our lawyers were worried because it turns out the company inherits its debt from the parent universe, but luckily cosmic inflation reduced it to nearly zero.', 'alt': 'Helium Synthesis', 'srcset': '//imgs.xkcd.com/comics/helium_synthesis_2x.png 2x', 'style': 'image-orientation:none'}\n",
      "Downloading image https://imgs.xkcd.com/comics/helium_synthesis.png...\n",
      "\n",
      "Downloading page https://xkcd.com/2971/...\n",
      "Image: <img alt=\"Celestial Event\" src=\"//imgs.xkcd.com/comics/celestial_event.png\" srcset=\"//imgs.xkcd.com/comics/celestial_event_2x.png 2x\" style=\"image-orientation:none\" title=\"If we can get a brood of 13-year cicadas going, we might have a chance at making this happen before the oceans evaporate under the expanding sun.\"/>\n",
      "Image attr: {'src': '//imgs.xkcd.com/comics/celestial_event.png', 'title': 'If we can get a brood of 13-year cicadas going, we might have a chance at making this happen before the oceans evaporate under the expanding sun.', 'alt': 'Celestial Event', 'srcset': '//imgs.xkcd.com/comics/celestial_event_2x.png 2x', 'style': 'image-orientation:none'}\n",
      "Downloading image https://imgs.xkcd.com/comics/celestial_event.png...\n",
      "\n",
      "Downloading page https://xkcd.com/2970/...\n",
      "Image: <img alt=\"Meteor Shower PSA\" src=\"//imgs.xkcd.com/comics/meteor_shower_psa.png\" srcset=\"//imgs.xkcd.com/comics/meteor_shower_psa_2x.png 2x\" style=\"image-orientation:none\" title=\"If you hold the meteor too long, it may imprint on you and form a contact binary, making reintroduction to space difficult.\"/>\n",
      "Image attr: {'src': '//imgs.xkcd.com/comics/meteor_shower_psa.png', 'title': 'If you hold the meteor too long, it may imprint on you and form a contact binary, making reintroduction to space difficult.', 'alt': 'Meteor Shower PSA', 'srcset': '//imgs.xkcd.com/comics/meteor_shower_psa_2x.png 2x', 'style': 'image-orientation:none'}\n",
      "Downloading image https://imgs.xkcd.com/comics/meteor_shower_psa.png...\n",
      "\n",
      "Downloading page https://xkcd.com/2969/...\n",
      "Image: <img alt=\"Vice President First Names\" src=\"//imgs.xkcd.com/comics/vice_president_first_names.png\" srcset=\"//imgs.xkcd.com/comics/vice_president_first_names_2x.png 2x\" style=\"image-orientation:none\" title='[Political pundit on the ScrabbleTV News channel] \"After four years of defying orthographic pressure, Joe ceded the top of the ticket to Kamala, who--after considering Josh, Mark, Andy, Roy, and Pete--picked Tim.\"'/>\n",
      "Image attr: {'src': '//imgs.xkcd.com/comics/vice_president_first_names.png', 'title': '[Political pundit on the ScrabbleTV News channel] \"After four years of defying orthographic pressure, Joe ceded the top of the ticket to Kamala, who--after considering Josh, Mark, Andy, Roy, and Pete--picked Tim.\"', 'alt': 'Vice President First Names', 'srcset': '//imgs.xkcd.com/comics/vice_president_first_names_2x.png 2x', 'style': 'image-orientation:none'}\n",
      "Downloading image https://imgs.xkcd.com/comics/vice_president_first_names.png...\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://xkcd.com'\n",
    "# Create a directory to store the comics\n",
    "os.makedirs('xkcd', exist_ok=True)\n",
    "\n",
    "# Download the first 5 comics\n",
    "NUM_COMICS = 5\n",
    "for i in range(NUM_COMICS):\n",
    "    # Download the page\n",
    "    print(f'Downloading page {url}...')\n",
    "    page = requests.get(url)\n",
    "    page.raise_for_status()\n",
    "\n",
    "    # print(f\"Page content:\\n {page.text[:100]}\")\n",
    "\n",
    "    # Find the URL of the comic image using 'id'='comic'\n",
    "    soup = bs4.BeautifulSoup(page.text, 'html.parser')\n",
    "    images = soup.select('#comic img')\n",
    "    image = images[0]\n",
    "    print(f\"Image: {image}\")\n",
    "    print(f\"Image attr: {image.attrs}\")\n",
    "    comic_url = 'https:' + image.get('src')\n",
    "\n",
    "    # Download the comic image\n",
    "    print(f'Downloading image {comic_url}...')\n",
    "    # image = requests.get(comic_url)\n",
    "    # image.raise_for_status()\n",
    "\n",
    "    # Save the image to the directory ./xkcd\n",
    "    # with open(os.path.join('xkcd', os.path.basename(comic_url)), 'wb') as f:\n",
    "    #     f.write(image.content)\n",
    "\n",
    "    # Get the Prev button's url\n",
    "    prev_link = soup.select('a[rel=\"prev\"]')[0]\n",
    "    url = 'https://xkcd.com' + prev_link.get('href')\n",
    "    print()\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python books from Nostarch Press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
